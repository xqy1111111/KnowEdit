import json, os, re, torch
from typing import List, Dict, Any, Iterable, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM

# ---------- IO ----------
def load_mquake_json(path: str) -> List[Dict[str, Any]]:
    """æ”¯æŒ .json / .jsonl"""
    with open(path, "r", encoding="utf-8") as f:
        head = f.read(1); f.seek(0)
        if head == "[":  # JSON
            return json.load(f)
        else:            # JSONL
            return [json.loads(line) for line in f if line.strip()]

# ---------- æ–‡æœ¬ä¸åŒ¹é…ï¼ˆæ”¯æŒ emojiï¼‰ ----------
def normalize_text(s: str) -> str:
    # ä»…ä¿ç•™å­—æ¯æ•°å­—ä¸ç©ºæ ¼ï¼Œå…¶ä»–å½’ä¸€ä¸ºç©ºæ ¼ï¼Œä¾¿äºåšè¯è¾¹ç•ŒåŒ¹é…
    return re.sub(r"\W+", " ", (s or "").strip().lower())

def sanitize_aliases(aliases: Iterable[str]) -> list:
    """
    è¿‡æ»¤å®¹æ˜“è¯¯åˆ¤çš„åˆ«åï¼š
    - æ–‡æœ¬å½’ä¸€åŒ–åé•¿åº¦ <= 2 ä¸”å…¨å­—æ¯ï¼ˆå¦‚ it, us, ukï¼‰ä¸¢å¼ƒ
    - emoji / å«éå­—æ¯å­—ç¬¦ï¼ˆæ¯”å¦‚ 'ğŸ‡®ğŸ‡¹', 'U.S.'ï¼‰ä¿ç•™
    """
    safe = []
    for n in aliases:
        if not n:
            continue
        g = normalize_text(n)
        if g and len(g) <= 2 and g.isalpha():
            continue
        safe.append(n)
    return safe

def contains_any(hay: str, needles: Iterable[str]) -> bool:
    """
    - å­—è¯ç±»åˆ«åï¼šåœ¨ normalize åæ–‡æœ¬ä¸Šåšè¯è¾¹ç•ŒåŒ¹é…
    - éå­—è¯ï¼ˆemoji/æ ‡ç‚¹åºåˆ—ç­‰ï¼Œnormalize åä¸ºç©ºï¼‰ï¼šåœ¨åŸæ–‡ä¸ŠåšåŸæ ·åŒ¹é…
    """
    raw = hay or ""
    H = normalize_text(raw)
    for n in needles:
        if not n:
            continue
        g = normalize_text(n)
        if g:  # è¯è¾¹ç•ŒåŒ¹é…
            if re.search(rf"(?:^|\s){re.escape(g)}(?:\s|$)", H):
                return True
        else:  # çº¯ emoji / æ ‡ç‚¹ï¼šåŸæ ·åŒ¹é…
            if n in raw:
                return True
    return False

# ---------- HF æ¨¡å‹åŠ è½½ ----------
def load_hf_model(model_name_or_path: str, dtype="bfloat16"):
    tok = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
    mdl = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        torch_dtype=getattr(torch, dtype) if isinstance(dtype, str) else dtype,
        device_map="auto",
        trust_remote_code=True,
    )
    return tok, mdl

def free_cuda(*objs):
    for o in objs:
        try: del o
        except: pass
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# ---------- ç”Ÿæˆï¼šæ”¯æŒâ€œä»…æœ€ç»ˆç­”æ¡ˆ / æ˜¾ç¤ºæ¨ç†+æœ€ç»ˆç­”æ¡ˆâ€ ----------
def _build_messages(prompt: str, reveal_reasoning: bool) -> list:
    if reveal_reasoning:
        # ç”¨æ ‡ç­¾è¾…åŠ©è§£æ
        sys_msg = (
            "You are a helpful assistant. "
            "Think step by step INSIDE <reasoning>...</reasoning>. "
            "Then output ONLY the final short answer INSIDE <final>...</final>. "
            "Do not include anything else outside these tags."
        )
    else:
        sys_msg = "Only output the final short answer. Do NOT explain."
    return [
        {"role": "system", "content": sys_msg},
        {"role": "user",   "content": prompt}
    ]

def generate_answer(model, tokenizer, prompt: str,
                    max_new_tokens=128, temperature=0.0, top_p=1.0,
                    reveal_reasoning: bool = False) -> str:
    """
    å¯¹ Instruct/Chat æ¨¡å‹ä¼˜å…ˆèµ° chat æ¨¡æ¿ï¼›
    å¯¹ base æ¨¡å‹é€€åŒ–ä¸ºæ™®é€šç»­å†™ã€‚
    â€”â€” ä¸åšâ€œé¦–è¡Œå³åœâ€ç­‰å¼ºæˆªæ–­ï¼›æ˜¯å¦é•¿è¾“å‡ºäº¤ç»™è°ƒç”¨å¤„çš„ max_new_tokens æ§åˆ¶ã€‚
    """
    # ç»Ÿä¸€ pad_tokenï¼Œæ¶ˆé™¤è­¦å‘Š
    if tokenizer.pad_token_id is None:
        if tokenizer.eos_token_id is not None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
        else:
            if tokenizer.unk_token_id is None:
                tokenizer.add_special_tokens({"unk_token": "<unk>"})
            tokenizer.pad_token_id = tokenizer.unk_token_id
    try:
        model.generation_config.pad_token_id = tokenizer.pad_token_id
    except Exception:
        pass

    messages = _build_messages(prompt, reveal_reasoning=reveal_reasoning)

    if hasattr(tokenizer, "apply_chat_template"):
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        # é€€åŒ–åˆ°çº¯æ–‡æœ¬
        text = (messages[0]["content"] + "\nQ: " + prompt + "\nA:")

    enc = tokenizer(text, return_tensors="pt").to(model.device)

    # æ”¶é›†å¯èƒ½çš„ EOS
    eos_ids = []
    for tok in ["<|im_end|>", "<|end|>", tokenizer.eos_token]:
        try:
            tid = tokenizer.convert_tokens_to_ids(tok) if tok else None
        except Exception:
            tid = None
        if isinstance(tid, int) and tid >= 0:
            eos_ids.append(tid)
    if not eos_ids and tokenizer.eos_token_id is not None:
        eos_ids = [tokenizer.eos_token_id]

    out = model.generate(
        **enc,
        max_new_tokens=max_new_tokens,
        do_sample=(temperature > 0),
        temperature=temperature,
        top_p=top_p,
        eos_token_id=eos_ids[0] if len(eos_ids) == 1 else eos_ids,
        no_repeat_ngram_size=3  # ç¨æŠ‘åˆ¶è‡ªè¿°é‡å¤
    )
    ans = tokenizer.decode(out[0][enc.input_ids.shape[1]:], skip_special_tokens=True)
    return ans.strip()

# ---------- è§£æï¼šæŠŠæ¨ç†ä¸æœ€ç»ˆç­”æ¡ˆåˆ†å¼€ ----------
_TAG_REASON = re.compile(r"<reasoning>\s*(.*?)\s*</reasoning>", re.IGNORECASE | re.DOTALL)
_TAG_FINAL   = re.compile(r"<final>\s*(.*?)\s*</final>",         re.IGNORECASE | re.DOTALL)

def _clean(s: str) -> str:
    return (s or "").strip().strip("ï¼š:").strip()

def split_reasoning_final(text: str) -> Tuple[str, str]:
    """
    è¿”å› (final, reasoning)
    è§£æä¼˜å…ˆçº§ï¼š
      1) <reasoning>...</reasoning> ä¸ <final>...</final>
      2) <think>...</think> + 'Final:' / 'æœ€ç»ˆç­”æ¡ˆï¼š'
      3) åªå‡ºç° 'Final:' / 'æœ€ç»ˆç­”æ¡ˆï¼š'
      4) å…œåº•ï¼šæŠŠæœ«è¡Œå½“ finalï¼Œå…¶ä½™å½“ reasoning
    """
    raw = text or ""

    # 1) XML æ ‡ç­¾
    m_f = _TAG_FINAL.search(raw)
    m_r = _TAG_REASON.search(raw)
    if m_f:
        final = _clean(m_f.group(1))
        reasoning = _clean(m_r.group(1)) if m_r else _clean(_TAG_REASON.sub("", raw))
        return final, reasoning

    # 2) DeepSeek å¸¸è§ <think>... </think> + Final:
    m_think = re.search(r"<think>\s*(.*?)\s*</think>", raw, re.IGNORECASE | re.DOTALL)
    m_final_line = re.search(r"(?im)^\s*(final(?: answer)?|æœ€ç»ˆç­”æ¡ˆ)\s*[:ï¼š]\s*(.+)\s*$", raw)
    if m_final_line:
        final = _clean(m_final_line.group(2))
        reasoning = _clean(m_think.group(1)) if m_think else _clean(raw[:m_final_line.start()])
        return final, reasoning

    # 3) åªæœ‰ Final: / æœ€ç»ˆç­”æ¡ˆï¼š
    m_final_alone = re.search(r"(?im)(final(?: answer)?|æœ€ç»ˆç­”æ¡ˆ)\s*[:ï¼š]\s*(.+)$", raw)
    if m_final_alone:
        return _clean(m_final_alone.group(2)), _clean(raw[:m_final_alone.start()])

    # 4) å…œåº•ï¼šå–æœ€åä¸€è¡Œæˆ–æœ€åä¸€å¥ä½œä¸º final
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    if lines:
        final_guess = lines[-1]
        reasoning_guess = "\n".join(lines[:-1])
        return _clean(final_guess), _clean(reasoning_guess)

    return "", ""  # ç©ºè¾“å‡º
